{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** \n",
    "\n",
    "In this tutorial we will create a simple gravity problem from scratch using the SimPEG framework. The synthetic example used here is based on the Tli Kwi Cho kimberlite deposit, NWT. We are attempting to use the gravity field measurements to image various kimberlite rock units making up the diamond deposit.\n",
    "\n",
    "Since both gravity and magnetic surveys are very similar (both potentials), we will use this example to showcase the power of geological constraints. By addind the information from a single borehole, we greatly reduce the none-uniqueness of the inverse problem and better recover density structures at depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Need to be on \\pf\\dev branch !!!\n",
    "import SimPEG.PF as PF\n",
    "from SimPEG import *\n",
    "from SimPEG.Utils import io_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "import time as tm\n",
    "import os\n",
    "from ipywidgets.widgets import interact, IntSlider\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "psep = os.path.sep\n",
    "url = 'https://storage.googleapis.com/simpeg/tkc_synthetic/potential_fields/'\n",
    "cloudfiles = ['SimPEG_Grav_Input.inp', 'Mesh.msh', 'GravData.obs', 'Initm.den']\n",
    "\n",
    "basePath = os.path.sep.join(os.path.abspath(os.getenv('HOME')).split(os.path.sep)+['Downloads']+['SimPEGtemp']) #hide\n",
    "basePath = io_utils.remoteDownload(url, cloudfiles, basePath=basePath+os.path.sep)\n",
    "input_file = basePath + 'SimPEG_Grav_Input.inp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the input file which included all parameters at once (mesh, topo, model, survey, inv param, etc.)\n",
    "driver = PF.GravityDriver.GravityDriver_Inv(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We already have created a mesh, model and survey for this example.\n",
    "# All the elements are stored in the driver, and can be accessed like this:\n",
    "mesh = driver.mesh\n",
    "survey = driver.survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "The relation between density and the gravity field is well known, thanks to the classic work of Newton in 1686. Since we generally only measure the vertical component of the field, this relationship can be written as:\n",
    "$$G(r)_z = \\gamma \\int_{V}   \\rho(r) \\left(\\frac{z - z_0}{{|\\vec r - \\vec r_0|}^3}\\right) \\; dV $$\n",
    "where $\\rho$ is the anomalous density and $\\gamma$ is the Newton's gravitational constant.\n",
    "Once again, this integral can be evaluated analytically for simple prisms, giving rise to a linear system of equations relating a discrete Earth to the observed data:|\n",
    "$$ \\mathbf{d}_z =  \\mathbf{F} \\; \\boldsymbol{\\rho} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We did not include a topography file in this example as the information\n",
    "# about inactive cells is already captured in our starting model.\n",
    "# Line 6 of the input file specifies a VALUE to be used as inactive flag.\n",
    "\n",
    "# Get the active cells\n",
    "actv = driver.activeCells\n",
    "nC = len(actv) # Number of active cells\n",
    "ndv = -100\n",
    "\n",
    "# Create active map to return to full space after the inversion \n",
    "actvMap = Maps.InjectActiveCells(mesh, actv, ndv)\n",
    "\n",
    "# Create a reduced identity map for the inverse problem\n",
    "idenMap = Maps.IdentityMap(nP=nC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward system:**\n",
    "\n",
    "Now that we have all our spatial components, we can create our linear system. For a single location and single component of the data, \n",
    "#Need to add stuff\n",
    "\n",
    "the system end up looking like this system:\n",
    "\n",
    "$$ d^{obs} = \\mathbf{F\\; \\rho}$$\n",
    "\n",
    "where $\\mathbf{F} \\in \\mathbb{R}^{nd \\times nc}$ is our $forward$ operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that we have a model and a survey we can build the linear system ...\n",
    "# Create the forward model operator (the argument forwardOnly=False store the forward matrix to memory)\n",
    "prob = PF.Gravity.GravityIntegral(mesh, mapping=idenMap, actInd=actv, forwardOnly=False, rtype = 'z')\n",
    "\n",
    "# Pair the survey and problem\n",
    "survey.pair(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fist time that we ask for predicted data,\n",
    "# the dense matrix T is calculated.\n",
    "# This is generally the bottleneck of the integral formulation in terms of cost\n",
    "d = prob.fields(driver.m0)\n",
    "\n",
    "# Add noise to the data and assign uncertainties\n",
    "data = d + np.random.randn(len(d))*0.01 # We add some random Gaussian noise (1nT)\n",
    "wd = np.ones(len(data))*np.sqrt(0.01) # Assign flat uncertainties\n",
    "\n",
    "survey.dobs = data\n",
    "survey.std = wd\n",
    "\n",
    "# [OPTIONAL] You can write the observations to UBC format here\n",
    "#PF.Magnetics.writeUBCobs('MAG_Synthetic_data.obs',survey,data)\n",
    "PF.Gravity.plot_obs_2D(survey.srcField.rxList[0].locs,d=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inverse problem**\n",
    "\n",
    "We have generated synthetic data, we now what to see if we can solve the inverse. Using the usual formulation, we seek a model that can reproduce the data, let’s say a least-squares measure of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_d =   \\|\\mathbf{W}_d \\left( \\mathbb{F}[\\mathbf{m}] - \\mathbf{d}^{obs} \\right)\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "The inverse problem is hard because we don’t have great data coverage, and the Earth is big, and there is usually noise in the data. So we need to add something to regularize it.\n",
    "The simplest way to do it is to penalize solutions that won’t make sense geologically, for example to assume that the model is small.\n",
    "The usual smooth inversion function use an l2-norm measure:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_d =   \\|\\mathbf{W}_d \\left( \\mathbb{F}[\\mathbf{m}] - \\mathbf{d}^{obs} \\right)\\|_2^2 \\\\\n",
    "\\phi_m = \\beta \\Big [ {\\| \\mathbf{W}_s \\;( \\mathbf{m - m^{ref}})\\|}^2_2  + \\sum_{i = x,y,z}  {\\|   \\mathbf{W}_i  \\; \\mathbf{G}_i \\; \\mathbf{m}\\|}^2_2  \\Big ]\\;,\n",
    "\\end{equation}\n",
    "\n",
    "The full objective function to be minimized can be written as:\n",
    "\\begin{equation}\n",
    "\\phi(m) =  \\phi_d + \\beta \\phi_m\\;,\n",
    "\\end{equation}\n",
    "which will yield our usual *small* and *smooth* models. \n",
    "\n",
    "We propose a fancier regularization function that can allow to recover *sparse* and *blocky* solutions.\n",
    "Starting with the well known Ekblom norm:\n",
    "\\begin{equation}\n",
    "\\phi_m =  \\sum_{i=1}^{nc} {(x_i^2 + \\epsilon^2)}^{p/2} \\;,\n",
    "\\end{equation}\n",
    "where $x_i$ denotes some function of the model parameter, and $\\epsilon$ is a small value to avoid singularity as $m\\rightarrow0$.\n",
    "For p=2, we get the usual least-squares measure and we recover the regularization presented above. For $p \\leq 1$, the function becomes non-linear which requires some tweaking.\n",
    "\n",
    "We can linearize the function by updating the penality function iteratively, commonly known as an Iterative Re-weighted Least-Squares (IRLS) method:\n",
    "\\begin{equation} \n",
    "\\phi_m^{(k)} =  \\frac{1}{2}\\sum_{i=1}^{nc} r_i \\; x_i^2\n",
    "\\end{equation}\n",
    "where we added the superscript $\\square^{(k)}$ to denote the IRLS iterations. The weights $r(x)$ are computed from model values obtained at a previous iteration such that:\n",
    "\\begin{equation}\n",
    "\t{r}_i  ={\\Big( {({x_i}^{(k-1)})}^{2} + \\epsilon^2 \\Big)}^{p/2 - 1} \\;,\n",
    "\\end{equation}\n",
    "where ${r}(x) \\in \\mathbb{R}^{nc}$.\n",
    "\n",
    "In matrix form, our objective function simply becomes:\n",
    "\\begin{equation}\n",
    "\\phi(m) =   \\|\\mathbf{W}_d \\left( \\mathbb{F}[\\mathbf{m}] - \\mathbf{d}^{obs} \\right)\\|_2^2 + \\beta \\Big [ {\\| \\mathbf{W}_s \\;\\mathbf{R}_s\\;( \\mathbf{m - m^{ref}})\\|}^2_2  + \\sum_{i = x,y,z}  {\\|   \\mathbf{W}_i\\; \\mathbf{R}_i  \\; \\mathbf{G}_i \\; \\mathbf{m}\\|}^2_2  \\Big ]\\;,\n",
    "\\end{equation}\n",
    "where the IRLS weights $\\mathbf{R}_s$ and $\\mathbf{R}_i$ are diagonal matrices defined as:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\t{R}_{s_{jj}}  &=  \\sqrt{\\eta_p}{\\Big[ {({m_j}^{(k-1)})}^{2} + \\epsilon_p^2 \\Big]}^{(p/2 - 1)/2} \\\\\n",
    "\t{R}_{i_{jj}}  &=  \\sqrt{\\eta_q}{\\Big[ {\\left ({{(G_i\\;m^{(k-1)})}_j }\\right)}^{2} + \\epsilon_q^2 \\Big]}^{(q/2 - 1)/2} \\\\\n",
    "\\eta_p &=  {\\epsilon_p}^{(1-p/2)} \\\\\n",
    "\\eta_q &=   {\\epsilon_q}^{(1-q/2)}  \\;, \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "we added two scaling parameters $\\eta_p$ and $\\eta_q$ for reasons that we won't dicuss here, but turn out to be important to get stable solves.\n",
    "\n",
    "In order to initialize the IRLS and get an estimate for the stabilizing parameters $\\epsilon_p$ and $\\epsilon_q$, we first invert with the smooth $l_2$-norm. \n",
    "The whole IRLS process is implemented with a directive added to the inversion workflow (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It is potential fields, so we will need to push the inverison down\n",
    "# Create distance weights from our linera forward operator\n",
    "wr = np.sum(prob.G**2.,axis=0)**0.5\n",
    "wr = ( wr/np.max(wr) )\n",
    "    \n",
    "reg = Regularization.Sparse(mesh, indActive=actv, mapping=idenMap)\n",
    "reg.cell_weights = wr\n",
    "\n",
    "dmis = DataMisfit.l2_DataMisfit(survey)\n",
    "dmis.Wd = 1/wd\n",
    "\n",
    "# Add directives to the inversion\n",
    "opt = Optimization.ProjectedGNCG(maxIter=100 ,lower=driver.bounds[0],upper=driver.bounds[1], maxIterLS = 20, maxIterCG= 10, tolCG = 1e-3)\n",
    "invProb = InvProblem.BaseInvProblem(dmis, reg, opt)\n",
    "betaest = Directives.BetaEstimate_ByEig()\n",
    "\n",
    "# Here is where the norms are applied\n",
    "# Use pick a treshold parameter empirically based on the distribution of model\n",
    "# parameters (run last cell to see the histogram before and after IRLS)\n",
    "IRLS = Directives.Update_IRLS( norms=driver.lpnorms,  eps=(1e-2,1e-2), f_min_change = 1e-4, minGNiter=3)\n",
    "update_Jacobi = Directives.Update_lin_PreCond()\n",
    "inv = Inversion.BaseInversion(invProb, directiveList=[IRLS,betaest,update_Jacobi])\n",
    "\n",
    "m0 = np.ones(idenMap.nP)*1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run inversion...\n",
    "mrec = inv.run(m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the final model back to full space and plot!!\n",
    "m_lp = actvMap*mrec\n",
    "m_lp[m_lp==ndv] = np.nan\n",
    "\n",
    "# Get the smooth model aslo\n",
    "m_l2 = actvMap*reg.l2model\n",
    "m_l2[m_l2==ndv] = np.nan\n",
    "\n",
    "m_true = actvMap*driver.m0\n",
    "m_true[m_true==ndv] = np.nan\n",
    "#[OPTIONAL] Save both models to file\n",
    "#Mesh.TensorMesh.writeModelUBC(mesh,'SimPEG_MAG_l2l2.sus',m_l2)\n",
    "#Mesh.TensorMesh.writeModelUBC(mesh,'SimPEG_MAG_lplq.sus',m_lp)\n",
    "\n",
    "# Plot the recoverd models \n",
    "vmin, vmax = -.3, 0.05\n",
    "\n",
    "mesh = Mesh.TensorMesh([mesh.hx, mesh.hy, mesh.hz],x0=\"CCN\")\n",
    "\n",
    "def slide(s,normal):\n",
    "    \n",
    "    if normal == \"Z\":\n",
    "        fig = plt.figure(figsize=(10*1.2, 8))\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(10*1.2, 4))\n",
    "        \n",
    "    ax1 = plt.subplot(2,2,3)\n",
    "    dat = mesh.plotSlice(m_lp, ax = ax1, normal=normal, ind=s, clim=np.r_[vmin, vmax],pcolorOpts={'cmap':'viridis'})\n",
    "#     plt.colorbar(dat[0])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title('Compact model')\n",
    "    \n",
    "    if normal == \"Z\":\n",
    "        plt.xlim(-600, 600)\n",
    "        plt.ylim(-600, 600.)    \n",
    "    else:\n",
    "        plt.xlim(-600, 600)\n",
    "        plt.ylim(-500, 0.) \n",
    "        \n",
    "    ax2 = plt.subplot(2,2,1)\n",
    "    dat = mesh.plotSlice(m_l2, ax = ax2, normal=normal, ind=s, clim=np.r_[vmin, vmax],pcolorOpts={'cmap':'viridis'})\n",
    "#     plt.colorbar(dat[0])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title('Smooth model')\n",
    "    \n",
    "    if normal == \"Z\":\n",
    "        plt.xlim(-600, 600)\n",
    "        plt.ylim(-600, 600.)    \n",
    "    else:\n",
    "        plt.xlim(-600, 600)\n",
    "        plt.ylim(-500, 0.) \n",
    "        \n",
    "    ax2.set_xticklabels([])\n",
    "        \n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "    dat = mesh.plotSlice(m_true, ax = ax2, normal=normal, ind=s, clim=np.r_[vmin, vmax],pcolorOpts={'cmap':'viridis'})\n",
    "#     plt.colorbar(dat[0])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title('True model')\n",
    "    \n",
    "    pos =  ax2.get_position()\n",
    "\n",
    "    ax2.yaxis.set_visible(False)\n",
    "    if normal == \"Z\":\n",
    "        plt.xlim(-600, 600)\n",
    "        plt.ylim(-600, 600.) \n",
    "        ax2.set_position([pos.x0 -0.04 , pos.y0,  pos.width, pos.height])\n",
    "    else:\n",
    "        plt.xlim(-600, 600)\n",
    "        plt.ylim(-500, 0.) \n",
    "\n",
    "    pos =  ax2.get_position()\n",
    "    cbarax = fig.add_axes([pos.x0 + 0.375 , pos.y0 + 0.05,  pos.width*0.1, pos.height*0.75])  ## the parameters are the specified position you set\n",
    "    cb = fig.colorbar(dat[0],cax=cbarax, orientation=\"vertical\", ax = ax2, ticks=np.linspace(vmin,vmax, 4))\n",
    "    cb.set_label(\"Density (g/cc)\",size=12)\n",
    "    \n",
    "    #{OPTIONAL} Save the figure to png\n",
    "    #fig.savefig('PF_Compact.png',dpi = 150)\n",
    "    \n",
    "interact(slide, s=(0,34), normal=['Y','Z','X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up the working directory\n",
    "shutil.rmtree(basePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
